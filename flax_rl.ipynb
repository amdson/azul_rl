{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp                # JAX NumPy\n",
    "\n",
    "from flax import linen as nn           # The Linen API\n",
    "from flax import traverse_util\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "import numpy as np                     # Ordinary NumPy\n",
    "import optax                           # Optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def init_board():\n",
    "  return np.zeros((3, 3), dtype=int).flatten()\n",
    "\n",
    "def flip_board(board):\n",
    "  return -board\n",
    "\n",
    "def get_valid_mask(board):\n",
    "  return board == 0\n",
    "\n",
    "# #Always assume action is made by player 1\n",
    "# def get_next_state(board, action):\n",
    "#   return board.flatten().at[action].add(1).reshape(board.shape)\n",
    "\n",
    "#Always assume action is made by player 1\n",
    "#Assume action is valid\n",
    "def get_next_state(board, action):\n",
    "  next_board = board.copy()\n",
    "  next_board[action] = 1\n",
    "  return next_board\n",
    "\n",
    "def sample_action(action_dist, rng):\n",
    "  action_dist = action_dist.flatten() / action_dist.sum()\n",
    "  return jax.random.choice(rng, jnp.arange(action_dist.shape[0]), p=action_dist)\n",
    "\n",
    "def disp_board(board):\n",
    "  plt.imshow(board)\n",
    "\n",
    "reward_conv = nn.Conv(features=4, kernel_size=(3, 3), use_bias=False, padding='SAME')\n",
    "stripe_filter = jnp.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]])\n",
    "reward_kernel = jnp.expand_dims(jnp.stack([jnp.eye(3), jnp.eye(3)[::-1, :], stripe_filter, stripe_filter.T], axis=2), 2)\n",
    "\n",
    "reward_conv_param = {'params': {'kernel': reward_kernel}}\n",
    "# print(jnp.expand_dims(jnp.eye(4), (0, 3)))\n",
    "\n",
    "def get_reward(board):\n",
    "    board_score = reward_conv.apply(reward_conv_param, jnp.expand_dims(jnp.array(board).reshape((3, 3)), (0, 3)))\n",
    "    # print(jnp.transpose(board_score[0]))\n",
    "    is_win = (jnp.max(board_score) >= 3).astype(int)\n",
    "    is_loss = (jnp.min(board_score) <= -3).astype(int)\n",
    "    score = float(is_win - is_loss)\n",
    "    return score, score or (board == 0).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[-1  1 -1 -1  1 -1 -1  1  1] 0.0 True\n"
     ]
    }
   ],
   "source": [
    "board = init_board()\n",
    "print(board.reshape((3, 3)))\n",
    "\n",
    "rng = jax.random.PRNGKey(40)\n",
    "for i in range(10):\n",
    "  _, rng = jax.random.split(rng, 2)\n",
    "  action_dist = jnp.ones(board.shape) * get_valid_mask(board)\n",
    "  next_action = sample_action(action_dist, rng)\n",
    "  board = flip_board(get_next_state(board, next_action))\n",
    "  reward, game_over = get_reward(board)\n",
    "\n",
    "print(board, reward, game_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_model(state):\n",
    "    return np.ones(state.shape), 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n",
      "Finished mcts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "STATE_DIM = 9 #Dimension of 4x4 tic-tac-toe board\n",
    "ACTION_DIM = 9\n",
    "MAX_SIZE = int(1e3)\n",
    "C_BASE, C_INIT = 1.0, 1.0\n",
    "class MCTS:\n",
    "    def __init__(self):\n",
    "        self.state = np.zeros((MAX_SIZE, STATE_DIM))\n",
    "        self.state_lookup = {} #Maps state representation to index\n",
    "        self.expanded = []\n",
    "\n",
    "        self.visit_count = np.zeros(MAX_SIZE)\n",
    "        self.action_visits  = np.zeros((MAX_SIZE, ACTION_DIM), dtype=int)\n",
    "        self.action_total_value = np.zeros((MAX_SIZE, ACTION_DIM))\n",
    "        self.action_mean_value = np.zeros((MAX_SIZE, ACTION_DIM))\n",
    "        self.action_prior = np.zeros((MAX_SIZE, ACTION_DIM))\n",
    "        # self.action_children = np.zeros((MAX_SIZE, ACTION_DIM), dtype=int) - 1\n",
    "    \n",
    "    #Assumes state is already expanded, and uses MCTS info to pick best action\n",
    "    def select_action(self, state, state_index):\n",
    "        state_visits = self.visit_count[state_index]\n",
    "        exp_rate = np.log((1+state_visits + C_BASE)/C_BASE) + C_INIT\n",
    "        model_prior = self.action_prior[state_index]\n",
    "        sa_visits = self.action_visits[state_index]\n",
    "        sa_mean_value = self.action_mean_value[state_index]\n",
    "        action_distr = (sa_mean_value + exp_rate*np.sqrt(state_visits)*model_prior/(1+sa_visits))*get_valid_mask(state)\n",
    "        return np.argmax(action_distr)\n",
    "\n",
    "    def get_action_prob(self, state_index, temperature=1):\n",
    "        \"\"\"\n",
    "        Select action according to the visit count distribution and the temperature.\n",
    "        \"\"\"\n",
    "        action_visits = self.action_visits[state_index]\n",
    "        if temperature == 0:\n",
    "            a = np.argmax(action_visits)\n",
    "            r = np.zeros(action_visits.shape)\n",
    "            r[a] = 1.0\n",
    "            return r\n",
    "        elif temperature == float(\"inf\"):\n",
    "            return np.ones(action_visits.shape)/action_visits.shape[0]\n",
    "        else:\n",
    "            # See paper appendix Data Generation\n",
    "            visit_count_distribution = np.power(action_visits, 1 / temperature)\n",
    "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
    "            return visit_count_distribution\n",
    "            \n",
    "    def expand_node(self, state, action_probs):\n",
    "        state_index = len(self.expanded)\n",
    "        # print(\"state index\", state_index)\n",
    "        self.expanded.append(True)\n",
    "        self.state_lookup[state.tobytes()] = state_index\n",
    "\n",
    "        self.state[state_index] = state\n",
    "        # print(self.action_prior[state_index].shape, action_probs.shape)\n",
    "        self.action_prior[state_index] = action_probs\n",
    "        return state_index\n",
    "\n",
    "    def search_iter(self, state_index, model):\n",
    "        search_path = []\n",
    "        path_actions = []\n",
    "\n",
    "        curr_index = state_index\n",
    "        curr_state = self.state[state_index]\n",
    "\n",
    "        # Loop until reaching an untracked state\n",
    "        while curr_index >= 0:\n",
    "            search_path.append(curr_index)\n",
    "            action = self.select_action(curr_state, curr_index)\n",
    "            path_actions.append(action)\n",
    "\n",
    "            curr_state = flip_board(get_next_state(curr_state, action))\n",
    "            nsr = curr_state.tobytes()\n",
    "            if(nsr in self.state_lookup):\n",
    "                curr_index = self.state_lookup[nsr]\n",
    "            else:\n",
    "                curr_index = -1\n",
    "\n",
    "        # The value of the new state from the perspective of the other player\n",
    "        next_state = curr_state\n",
    "        value, game_over = get_reward(next_state)\n",
    "        value = -value\n",
    "        if not game_over:\n",
    "            # If the game has not ended:\n",
    "            # EXPAND\n",
    "            action_probs, value = model(next_state)\n",
    "            valid_moves = get_valid_mask(next_state)\n",
    "            action_probs = action_probs * valid_moves  # mask invalid moves\n",
    "            action_probs /= np.sum(action_probs)\n",
    "            self.expand_node(next_state, action_probs)\n",
    "        \n",
    "        #Backpropagate MCTS search path\n",
    "        for i in range(len(search_path)-1, -1, -1):\n",
    "            si, a = search_path[i], path_actions[i]\n",
    "            self.visit_count[si] += 1\n",
    "            self.action_visits[si, a] += 1\n",
    "            self.action_total_value[si, a] += value\n",
    "            self.action_mean_value[si, a] = self.action_total_value[si, a] / self.action_visits[si, a]\n",
    "            value *= -1\n",
    "\n",
    "        # print(\"search path\", search_path)\n",
    "        # print(\"search visits after update\", [self.visit_count[s] for s in search_path])\n",
    "        \n",
    "    def mcts_eval(self, state, model, num_sims):\n",
    "        # print(\"Expanded len\", len(self.expanded))\n",
    "        root_state = state\n",
    "        action_prior, value_est = model(root_state)\n",
    "        root_index = self.expand_node(root_state, action_prior)\n",
    "        # print(\"Root index\", root_index)\n",
    "        for _ in range(num_sims):\n",
    "            self.search_iter(root_index, model)\n",
    "        return root_index\n",
    "    \n",
    "    def print_tree(self, root_index=0):\n",
    "        for state_index in self.state_lookup.values():\n",
    "            state = self.state[state_index]\n",
    "            print(state.reshape((3, 3)))\n",
    "            visited_actions = self.action_visits[state_index].nonzero()[0]\n",
    "            action_values = self.action_mean_value[state_index, visited_actions]\n",
    "            print(\"Action\", visited_actions)\n",
    "            print(\"Action value\", action_values)\n",
    "\n",
    "    def visualize_tree(self):\n",
    "        G = nx.Graph()\n",
    "        node_labels = {}\n",
    "        edge_labels = {}\n",
    "        for state_index in self.state_lookup.values():\n",
    "            visited_actions = self.action_visits[state_index].nonzero()[0]\n",
    "            action_values = self.action_mean_value[state_index, visited_actions]\n",
    "            state = self.state[state_index]\n",
    "            for action, value in zip(visited_actions, action_values):\n",
    "                child_state = flip_board(get_next_state(state, action))\n",
    "                # print(child_state.reshape((4,4)))\n",
    "                child_index = self.state_lookup[child_state.tobytes()]\n",
    "                G.add_edge(state_index, child_index)\n",
    "                edge_labels[(state_index, child_index)] = value\n",
    "            node_labels[state_index] = f\"{state_index}, {self.action_visits[state_index].sum()}\" \n",
    "        \n",
    "        pos = nx.spring_layout(G)\n",
    "        plt.figure()\n",
    "        nx.draw(\n",
    "            G, pos, edge_color='black', width=1, linewidths=1, node_size=10, \n",
    "            node_color='pink', alpha=0.9,\n",
    "            # labels=node_labels\n",
    "        )\n",
    "        # nx.draw_networkx_edge_labels(\n",
    "        #     G, pos,\n",
    "        #     edge_labels=edge_labels,\n",
    "        #     font_color='red'\n",
    "        # )\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "root_state = init_board()\n",
    "root_state[1] = 1\n",
    "root_state[7] = 1\n",
    "print(root_state.reshape((3, 3)))\n",
    "# root_state[:2] = 1\n",
    "mcts = MCTS()\n",
    "mcts.mcts_eval(root_state, toy_model, 50)\n",
    "print(\"Finished mcts\")\n",
    "# mcts.visualize_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 3\n",
      "action 2\n",
      "action 8\n",
      "action 6\n",
      "action 7\n",
      "action 0\n",
      "action 1\n",
      "action 4\n"
     ]
    }
   ],
   "source": [
    "def self_play_episode(model, num_sims=50, temp_threshold=6):\n",
    "    train_examples = []\n",
    "    board = init_board()\n",
    "    step = 0\n",
    "    while True:\n",
    "        mcts = MCTS()\n",
    "        root_index = mcts.mcts_eval(board, model, num_sims=num_sims)\n",
    "        # print(root_index, mcts.state[root_index].reshape((3,3)))\n",
    "        # print(board.reshape((3,3)))\n",
    "        # print(mcts.state[0].reshape((3,3)))\n",
    "        # print(mcts.action_visits[0].reshape((3,3)))\n",
    "        \n",
    "        temp = int(step < temp_threshold)\n",
    "        pi = mcts.get_action_prob(root_index, temperature=temp)\n",
    "\n",
    "\n",
    "        train_examples.append((board, pi, step))\n",
    "\n",
    "        action = np.random.choice(ACTION_DIM, p=pi)\n",
    "        print(\"action\", action)\n",
    "\n",
    "        board = flip_board(get_next_state(board, action))\n",
    "        #Reward is always negative because board is flipped after move. \n",
    "        r, game_over = get_reward(board) \n",
    "        if(r != 0):\n",
    "            return [(b, p, (-1)**(step-s)) for b, p, s in train_examples]\n",
    "        step += 1\n",
    "\n",
    "def batch_examples(train_examples):\n",
    "    state_batch = jnp.stack([t[0] for t in train_examples])\n",
    "    pa_batch = jnp.stack([t[1] for t in train_examples])\n",
    "    r_batch = jnp.stack([t[2] for t in train_examples]).reshape((-1, 1))\n",
    "    return state_batch, pa_batch, r_batch\n",
    "\n",
    "train_examples = self_play_episode(toy_model)\n",
    "state_b, pa_b, r_b = batch_examples(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "               0.11111111, 0.11111111, 0.11111111, 0.11111111],\n",
       "              [0.07653027, 0.09736775, 0.07992052, 0.13066143, 0.15686639,\n",
       "               0.10830156, 0.10120884, 0.16185613, 0.08728711],\n",
       "              [0.08857683, 0.12516937, 0.10361782, 0.09504247, 0.12469787,\n",
       "               0.12033694, 0.11011213, 0.11450323, 0.11794329],\n",
       "              [0.13931678, 0.117563  , 0.10381063, 0.1656441 , 0.08276459,\n",
       "               0.05339085, 0.10845144, 0.11083571, 0.11822291],\n",
       "              [0.07295639, 0.13741463, 0.09114759, 0.05642284, 0.19417347,\n",
       "               0.15284966, 0.11017194, 0.10711964, 0.07774387],\n",
       "              [0.12512428, 0.09865574, 0.10199723, 0.23405725, 0.04898063,\n",
       "               0.07368465, 0.08672816, 0.13338219, 0.09738986],\n",
       "              [0.08199129, 0.11197416, 0.11482912, 0.05512401, 0.2190445 ,\n",
       "               0.13658142, 0.11735156, 0.05933076, 0.10377317],\n",
       "              [0.12995099, 0.12875842, 0.09874451, 0.23088002, 0.02833561,\n",
       "               0.05432   , 0.07549422, 0.15185624, 0.10165998]],            dtype=float32),\n",
       " DeviceArray([[ 0.0000000e+00],\n",
       "              [-9.8968232e-03],\n",
       "              [-1.2519409e-01],\n",
       "              [ 7.0313834e-02],\n",
       "              [ 2.2713196e-02],\n",
       "              [-7.5789668e-02],\n",
       "              [-1.1346407e-04],\n",
       "              [-4.5667398e-01]], dtype=float32))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import jit\n",
    "\n",
    "class TTTModel(nn.Module):\n",
    "  \"\"\"A simple MLP model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Dense(features=64)(x)\n",
    "    body = nn.Dense(features=32)(x)\n",
    "    x = nn.Dense(features=9)(body)\n",
    "    value = nn.tanh(nn.Dense(features=1)(body)) #Value estimate between -1 and 1\n",
    "    return x, value\n",
    "\n",
    "model = TTTModel()\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "model.apply(params, state_b)[1].shape\n",
    "\n",
    "@jit\n",
    "def model_agent(x, params):\n",
    "  logits, value = TTTModel().apply(params, x)\n",
    "  return nn.softmax(logits), value\n",
    "\n",
    "model_agent(state_b, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate, momentum):\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  model = TTTModel()\n",
    "  params = model.init(rng, jnp.ones([1, 9]))['params']\n",
    "\n",
    "  tx = optax.sgd(learning_rate, momentum)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, state_b, pa_b, r_b):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits, exp_value = TTTModel().apply({'params': params}, state_b)\n",
    "    loss = optax.softmax_cross_entropy(logits, pa_b).mean() + jnp.square(r_b - exp_value).mean()\n",
    "    return loss, (logits, exp_value)\n",
    "  grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "  grads, aux = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  # metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "  return state#, metrics\n",
    "\n",
    "state = create_train_state(rng, 0.1, 0.1)\n",
    "state_b, pa_b, r_b = batch_examples(train_examples)\n",
    "state = train_step(state, state_b, pa_b, r_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([ 0.11060315,  0.39404774,  0.11323913,  0.15138267,\n",
       "               0.03048529, -0.2978174 ,  0.09324761,  0.09060495,\n",
       "               0.19323306], dtype=float32),\n",
       " DeviceArray([0.5425823], dtype=float32))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TTTModel()\n",
    "\n",
    "model.apply({'params': state.params}, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    return x\n",
    "\n",
    "cnn = CNN()\n",
    "params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1c2c9eb50c248e284b0bf4da0c34373df9866a584f49a897fbedf2615d148e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 ('azur_rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
