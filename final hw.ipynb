{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f694c8",
   "metadata": {},
   "source": [
    "# Playing Azul with Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf02e42",
   "metadata": {},
   "source": [
    "Deep reinforcement learning is a technique that combines deep neural network and reinforcement learning, and it could largely improve the accuracy and precision of the result of some policy-making problems. [A general reinforcement learning algorithm that masters chess, shogi and Go through self-play](https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd?fbclid=IwAR1CiRCE0a5nrZBQs2A2Ezw3fh3VUg7JWFC0m8ZKNDIp4xOzqPuhUmTgYQk) showed a groundbreaking achievement of reinforcement learning. Following questions will also be based on the methods of this paper but in new games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a336b9e",
   "metadata": {},
   "source": [
    "## 1. basic structure of deep reinforcement learning (Tic-Tac-Toe as example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02090561",
   "metadata": {},
   "source": [
    "For the approach described in the paper, the deep neural network learns a map:\n",
    "\n",
    "$$f(s; \\theta) = (p, v).$$\n",
    "        \n",
    "The map (with parameters $\\theta$) takes in a state $s$ and outputs a vector of move probabilities $p$ and an estimate of the outcome $v$. The vector of probabilities $p$ is called a *policy*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb73cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax\n",
    "!pip install flax\n",
    "!pip install optax\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp                # JAX NumPy\n",
    "\n",
    "from flax import linen as nn           # The Linen API\n",
    "from flax import traverse_util\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "import numpy as np                     # Ordinary NumPy\n",
    "import optax                           # Optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b70236",
   "metadata": {},
   "source": [
    "### 1) states function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258dba6f",
   "metadata": {},
   "source": [
    "You need to implement `get_next_state` and `get_reward` function.\n",
    "\n",
    "**Hint:** In this game, we will use 1 to represent player 1 and -1 for player 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869079d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_board():\n",
    "  return np.zeros((3, 3), dtype=int).flatten()\n",
    "\n",
    "def flip_board(board):\n",
    "  # used to change the side of pieces on the board according to current player\n",
    "  return -board\n",
    "\n",
    "def get_valid_mask(board):\n",
    "  return board == 0\n",
    "\n",
    "def get_next_state(board, action):\n",
    "  ################################################################################################\n",
    "  # TODO: Implement the next state results using current board and action.\n",
    "  # Hint: Always assume action is made by player 1\n",
    "  #################################################################################################\n",
    "  if( ? ):\n",
    "    print(\"Illegal Move\")\n",
    "    print(board.reshape((3,3)))\n",
    "    print(action)\n",
    "    print(get_reward(board))\n",
    "    assert False\n",
    "  next_board = board.copy()\n",
    "  next_board[action] = ?\n",
    "  ################################################################################################\n",
    "  # END OF YOUR CODE\n",
    "  ################################################################################################\n",
    "  return next_board\n",
    "\n",
    "def sample_action(action_dist):\n",
    "  # randomly choose an action\n",
    "  action_dist = action_dist.flatten() / action_dist.sum()\n",
    "  return np.random.choice(action_dist.shape[0], p=action_dist)\n",
    "\n",
    "def disp_board(board):\n",
    "  # used to display the board\n",
    "  plt.imshow(board)\n",
    "\n",
    "def get_reward(board):\n",
    "    ################################################################################################\n",
    "    # TODO: Implement the reward function.\n",
    "    #   We first calculate whether there is a win situation, including diagonal, row and\n",
    "    #   column situation. Then we return the reward and whether the game is end.\n",
    "    ################################################################################################\n",
    "    b = board.reshape((3,3))\n",
    "    diag = ? <= -3\n",
    "    diag_l = ? <= -3\n",
    "    row = ? <= -3\n",
    "    col = ? <= -3\n",
    "    reward = -int(diag or diag_l or row or col)\n",
    "    return reward, ?\n",
    "    ################################################################################################\n",
    "    # END OF YOUR CODE\n",
    "    ################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3142cc",
   "metadata": {},
   "source": [
    "Now we can test the basic structure of policy and action. You need to implement the key code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng = jax.random.PRNGKey(40)\n",
    "for i in range(200):\n",
    "  board = init_board()\n",
    "  for i in range(10):\n",
    "    \"\"\"\n",
    "    1) get all possible actions/positions (Hint: use get_valid_mask())\n",
    "    2) choose next action using sample_action()\n",
    "    3) get new state from current state and action, then change player's turn using flip_board()\n",
    "    4) get the reward and game status, and break from the game if it is end\n",
    "    \"\"\"\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "print(board.reshape((3, 3)), reward, game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c2a31d",
   "metadata": {},
   "source": [
    "This essay uses Monte Carlo Tree Search(MCTS) algorithm, which need to be implemented in the below.\n",
    "\n",
    "Each round of Monte Carlo tree search consists of four steps:\n",
    "\n",
    "**Selection**: Start from root $R$ and select successive child nodes until a leaf node $L$ is reached. The root is the current game state and a leaf is any node that has a potential child from which no simulation (playout) has yet been initiated.\n",
    "\n",
    "**Expansion**: Unless $L$ ends the game decisively (e.g. win/loss/draw) for either player, create one (or more) child nodes and choose node $C$ from one of them. Child nodes are any valid moves from the game position defined by $L$.\n",
    "\n",
    "**Simulation**: Complete one random playout from node $C$. This step is sometimes also called playout or rollout. A playout may be as simple as choosing uniform random moves until the game is decided (for example in chess, the game is won, lost, or drawn).\n",
    "\n",
    "**Backpropagation**: Use the result of the playout to update information in the nodes on the path from $C$ to $R$.\n",
    "\n",
    "Each state-action pair $(s, a)$ stores a set of statistics, $\\{N(s, a), W(s, a), Q(s, a), P(s, a)\\}$, where $N(s, a)$ is the visit count, $W(s, a)$ is the total action-value, $Q(s, a)$ is the mean action-value, and $P(s, a)$ is the prior probability of selecting $a$ in $s$. Each simulation begins at the root node of the search tree, $s_0$, and finishes when the simulation reaches a leaf node $s_L$ at time-step $L$. At each of these timesteps, $t < L$, an action is selected, $a_t = argmax_a(Q(s_t, a) + U(s_t, a))$, using a variant of the PUCT algorithm, $U(s, a) = C(s)P(s, a)\\sqrt{N(s)}/(1 + N(s, a))$, where $N(s)$ is\n",
    "the parent visit count and $C(s)$ is the exploration rate, which grows slowly with search time, $C(s) = log ((1 + N(s) + c_{base})/c_{base}) + c_{init}$, but is essentially constant during the fast training games.\n",
    "\n",
    "The stochastic policy obtained after performing the MCTS uses exponentiated counts, i.e. \n",
    "$$π(s)=N(s,⋅)^{1/τ}/∑_b(N(s,b)^{1/τ})$$, where $τ$ is the temperature and controls the degree of exploration. AlphaGo Zero uses $τ = 1$ (simply the normalised counts) for the first 30 moves of each game, and then sets it to an infinitesimal value (picking the move with the maximum counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9125997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_model(state):\n",
    "    return np.ones(state.shape) / state.shape[0], 0.1\n",
    "\n",
    "STATE_DIM = 9 #Dimension of 4x4 tic-tac-toe board\n",
    "ACTION_DIM = 9\n",
    "MAX_SIZE = int(1e3)\n",
    "C_BASE, C_INIT = 1.0, 1.0\n",
    "class MCTS:\n",
    "    def __init__(self, max_size=MAX_SIZE):\n",
    "        self.state = np.zeros((max_size, STATE_DIM))\n",
    "        self.state_lookup = {} #Maps state representation to index\n",
    "        self.expanded = []\n",
    "\n",
    "        self.visit_count = np.zeros(max_size)\n",
    "        self.action_visits  = np.zeros((max_size, ACTION_DIM), dtype=int)\n",
    "        self.action_total_value = np.zeros((max_size, ACTION_DIM))\n",
    "        self.action_mean_value = np.zeros((max_size, ACTION_DIM))\n",
    "        self.action_prior = np.zeros((max_size, ACTION_DIM))\n",
    "        # self.action_children = np.zeros((MAX_SIZE, ACTION_DIM), dtype=int) - 1\n",
    "    \n",
    "    #Assumes state is already expanded, and uses MCTS info to pick best action\n",
    "    def select_action(self, state, state_index):\n",
    "    ################################################################################################\n",
    "    # TODO: Implement the action selection.\n",
    "    #   1) get the distribution of all possible actions (Hint: using formulas given above)\n",
    "    #   2) choose the best action and return relevant information\n",
    "    ################################################################################################\n",
    "        state_visits = ?\n",
    "        exp_rate = 2\n",
    "        model_prior = ?\n",
    "        sa_visits = ?\n",
    "        sa_mean_value = ? #Normalize to [0, 1]\n",
    "        action_distr = ?\n",
    "        action = ?\n",
    "        if(state[action] != 0):\n",
    "            print(\"State\", state.reshape((3,3)))\n",
    "            print(\"action\", action)\n",
    "            print(\"exp rate\", exp_rate)\n",
    "            print(\"sa mean value\", sa_mean_value, sa_visits)\n",
    "            print(\"action distr\", action_distr)\n",
    "            print(\"model prior\", model_prior)\n",
    "            print(\"action_mask\", get_valid_mask(state))\n",
    "        return action\n",
    "    ################################################################################################\n",
    "    # END OF YOUR CODE\n",
    "    ################################################################################################\n",
    "\n",
    "    def get_action_prob(self, state_index, temperature=1):\n",
    "    ################################################################################################\n",
    "    # TODO: Calculate visit count distribution using the temperature.\n",
    "    #   Hint: using formulas given above\n",
    "    ################################################################################################\n",
    "        action_visits = self.action_visits[state_index]\n",
    "        visit_count_distribution = np.zeros(action_visits.shape)\n",
    "        if temperature == 0:\n",
    "            # START YOUR CODE\n",
    "            raise NotImplementedError\n",
    "            # END YOUR CODE\n",
    "        elif temperature == float(\"inf\"):\n",
    "            # START YOUR CODE\n",
    "            raise NotImplementedError\n",
    "            # END YOUR CODE\n",
    "        else:\n",
    "            # START YOUR CODE\n",
    "            raise NotImplementedError\n",
    "            # END YOUR CODE\n",
    "        return visit_count_distribution\n",
    "    ################################################################################################\n",
    "    # END OF YOUR CODE\n",
    "    ################################################################################################\n",
    "            \n",
    "    def expand_node(self, state, action_probs):\n",
    "    ################################################################################################\n",
    "    # TODO: expand the action probability of child nodes.\n",
    "    #   1) store expanded state\n",
    "    #   2) store all probabilities of valid moves\n",
    "    ################################################################################################\n",
    "        state_index = len(self.expanded)\n",
    "        self.expanded.append(True)\n",
    "        self.state_lookup[state.tobytes()] = ?\n",
    "        self.state[state_index] = ?\n",
    "        self.visit_count[state_index] += 1\n",
    "        valid_moves = ?\n",
    "        action_probs = ?  # mask invalid moves\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        self.action_prior[state_index] = action_probs\n",
    "        return state_index\n",
    "    ################################################################################################\n",
    "    # END OF YOUR CODE\n",
    "    ################################################################################################\n",
    "\n",
    "    def search_iter(self, state_index, model):\n",
    "        search_path = []\n",
    "        path_actions = []\n",
    "\n",
    "        curr_index = state_index\n",
    "        curr_state = self.state[state_index]\n",
    "        \n",
    "        ################################################################################################\n",
    "        # TODO: Loop until reaching an untracked state.\n",
    "        #   1) store each index to search_path and relative action to path_action\n",
    "        #   2) iterate next state to curr_state (Hint: remember to flip the board to change the turn)\n",
    "        #   3) find the index of next state and end the iteration if next index is not found\n",
    "        ################################################################################################\n",
    "        while curr_index >= 0:\n",
    "            # BEGIN YOUR CODE\n",
    "            raise NotImplementedError\n",
    "            # END YOUR CODE\n",
    "        \n",
    "        # The value of the new state from the perspective of the other player\n",
    "        next_state = curr_state\n",
    "        value, game_over = get_reward(next_state)\n",
    "        value = -value\n",
    "        ################################################################################################\n",
    "        # TODO: Expand the tree if the game has not ended.\n",
    "        #   1) get action_probs and value of next_state using given model\n",
    "        #   2) use masked and normalized action_prob to expand the node (Hint: use expand_node)\n",
    "        ################################################################################################\n",
    "        if not game_over:\n",
    "            # BEGIN YOUR CODE\n",
    "            raise NotImplementedError\n",
    "            # END YOUR CODE\n",
    "        \n",
    "        ################################################################################################\n",
    "        # TODO: Backpropagate MCTS search path.\n",
    "        #   1) get search index and action from search_path and path_actions\n",
    "        #   2) add relative value in visit_count, action_visits, action_total_value and \n",
    "        #      action_mean_value since this state and action are visited (Hint: use given variable 'value')\n",
    "        #   3) change the player (given in the code)\n",
    "        ################################################################################################\n",
    "        for i in range(len(search_path)-1, -1, -1):\n",
    "            # BEGIN YOUR CODE\n",
    "            raise NotImplementedError\n",
    "            # END YOUR CODE\n",
    "            value *= -1\n",
    "\n",
    "        # print(\"search path\", search_path)\n",
    "        # print(\"search visits after update\", [self.visit_count[s] for s in search_path])\n",
    "        ################################################################################################\n",
    "        # END OF YOUR CODE\n",
    "        ################################################################################################\n",
    "        \n",
    "    def mcts_eval(self, state, model, num_sims):\n",
    "        # print(\"Expanded len\", len(self.expanded))\n",
    "        root_state = state\n",
    "        action_prior, value_est = model(root_state)\n",
    "        root_index = self.expand_node(root_state, action_prior)\n",
    "        # print(\"Root index\", root_index)\n",
    "        for _ in range(num_sims):\n",
    "            self.search_iter(root_index, model)\n",
    "        return root_index\n",
    "     def print_tree(self, root_index=0):\n",
    "        for state_index in self.state_lookup.values():\n",
    "            state = self.state[state_index]\n",
    "            print(state.reshape((3, 3)))\n",
    "            visited_actions = self.action_visits[state_index].nonzero()[0]\n",
    "            action_values = self.action_mean_value[state_index, visited_actions]\n",
    "            print(\"Action\", visited_actions)\n",
    "            print(\"Action value\", action_values)\n",
    "\n",
    "    def visualize_tree(self):\n",
    "        G = nx.Graph()\n",
    "        node_labels = {}\n",
    "        edge_labels = {}\n",
    "        for state_index in self.state_lookup.values():\n",
    "            visited_actions = self.action_visits[state_index].nonzero()[0]\n",
    "            action_values = self.action_mean_value[state_index, visited_actions]\n",
    "            state = self.state[state_index]\n",
    "            for action, value in zip(visited_actions, action_values):\n",
    "                child_state = flip_board(get_next_state(state, action))\n",
    "                # print(child_state.reshape((3,3)))\n",
    "                if(child_state.tobytes() in self.state_lookup):\n",
    "                    child_index = self.state_lookup[child_state.tobytes()]\n",
    "                    G.add_edge(state_index, child_index)\n",
    "                    edge_labels[(state_index, child_index)] = f'{value:.2f}'\n",
    "            node_labels[state_index] = f\"{state_index}, {self.action_visits[state_index].sum()}\" \n",
    "        \n",
    "        pos = nx.spring_layout(G, scale=2.5)\n",
    "\n",
    "        plt.figure()\n",
    "        nx.draw(\n",
    "            G, pos, edge_color='black', width=1, linewidths=1, node_size=1000, \n",
    "            node_color='pink', alpha=0.9,\n",
    "            labels=node_labels\n",
    "        )\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            G, pos,\n",
    "            edge_labels=edge_labels,\n",
    "            font_color='red'\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_tree_detailed(self):\n",
    "        G = nx.Graph()\n",
    "        node_labels = {}\n",
    "        edge_labels = {}\n",
    "        for state_index in self.state_lookup.values():\n",
    "            visited_actions = self.action_visits[state_index].nonzero()[0]\n",
    "            action_values = self.action_mean_value[state_index, visited_actions]\n",
    "            state = self.state[state_index]\n",
    "            for action, value in zip(visited_actions, action_values):\n",
    "                child_state = flip_board(get_next_state(state, action))\n",
    "                # print(child_state.reshape((3,3)))\n",
    "                if(child_state.tobytes() in self.state_lookup):\n",
    "                    child_index = self.state_lookup[child_state.tobytes()]\n",
    "                    G.add_edge(state_index, child_index)\n",
    "                    edge_labels[(state_index, child_index)] = f'{value:.2f}'\n",
    "            node_labels[state_index] = f\"{state_index}, {self.action_visits[state_index].sum()}\" \n",
    "        \n",
    "        pos = nx.spring_layout(G)\n",
    "        plt.figure()\n",
    "        nx.draw(\n",
    "            G, pos, edge_color='black', width=1, linewidths=1, node_size=1000, \n",
    "            node_color='pink', alpha=0.9,\n",
    "            labels=node_labels\n",
    "        )\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            G, pos,\n",
    "            edge_labels=edge_labels,\n",
    "            font_color='red'\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f1fdf",
   "metadata": {},
   "source": [
    "Now we can try a simple situation to test whether our model works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396dee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_state = np.array([1, -1,  1, -1, 0,  0, -1,  0, 0])\n",
    "print(root_state.reshape((3,3)))\n",
    "mcts = MCTS(max_size=5000)\n",
    "mcts.mcts_eval(root_state, toy_model, 500)\n",
    "\n",
    "print(mcts.state[0].reshape((3,3)))\n",
    "print(mcts.action_mean_value[0].reshape((3,3)))\n",
    "print(len(mcts.expanded))\n",
    "mcts.visualize_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "data = board.reshape((3,3))\n",
    "\n",
    "# create discrete colormap\n",
    "cmap = colors.ListedColormap(['red', 'grey', 'blue'])\n",
    "bounds = [-2, -0.5, 0.5, 2]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# ax.imshow(data, cmap=cmap, norm=norm)\n",
    "\n",
    "def draw_layer(ax, to_print, layer_level, spacing):\n",
    "    #draw layer\n",
    "    gx = np.linspace(0, 0.6, 4)\n",
    "    draw_pos = []\n",
    "    for i, t in enumerate(to_print):\n",
    "        state, text = t\n",
    "        state = state.reshape((3,3))\n",
    "        ax.pcolormesh((gx+i*1.5)*spacing, gx*spacing-layer_level, state, shading='flat', cmap=cmap)\n",
    "        draw_pos.append((i*1.5*spacing, -layer_level))\n",
    "        ax.text((i*1.5+0.65)*spacing, -layer_level, text)\n",
    "    return draw_pos\n",
    "\n",
    "def plot_mcts(ax, mcts, root_index): \n",
    "    layer = [root_index]\n",
    "    value, count = mcts.action_mean_value[root_index].max(), mcts.visit_count[root_index]\n",
    "    to_print = [(mcts.state[root_index], f\"Visits: {count}\\nValue: {value:.2f}\")]\n",
    "\n",
    "    layer_row = 0\n",
    "    layer_level = 0\n",
    "    segments = []\n",
    "    while len(layer) > 0:\n",
    "        spacing = min(2 / len(to_print), 0.3)\n",
    "        prev_layer_level, layer_level = layer_level, layer_level + spacing, \n",
    "        prev_draw_pos = draw_layer(ax, to_print, layer_level, spacing)\n",
    "        print(prev_draw_pos)\n",
    "       \n",
    "        next_layer = []\n",
    "        to_print = []\n",
    "        seen_states = {}\n",
    "        for parent_i, state_index in enumerate(layer):\n",
    "            state = mcts.state[state_index]\n",
    "            visited_actions, = np.nonzero(mcts.action_visits[state_index])\n",
    "            visit_counts = mcts.action_visits[state_index][visited_actions]\n",
    "            action_values = mcts.action_mean_value[state_index, visited_actions]\n",
    "            for action, count, value in zip(visited_actions, visit_counts, action_values):\n",
    "                child_state = flip_board(get_next_state(state, action))\n",
    "                if(child_state.tobytes() in seen_states):\n",
    "                    child_i = seen_states[child_state.tobytes()]\n",
    "                    px, py = parent_i*1.5*spacing, -prev_layer_level\n",
    "                    cx, cy = child_i*1.5*spacing, -layer_level\n",
    "                    segments.append(((px, py), (cx, cy)))\n",
    "                    continue\n",
    "\n",
    "                if(child_state.tobytes() in mcts.state_lookup):\n",
    "                    child_index = mcts.state_lookup[child_state.tobytes()]\n",
    "                    next_layer.append(child_index)\n",
    "                # if(layer_row % 2 == 0):\n",
    "                #     child_state = flip_board(child_state)\n",
    "                    # value *= -1\n",
    "                \n",
    "                seen_states[child_state.tobytes()] = len(to_print)\n",
    "                child_i = len(to_print)\n",
    "                px, py = parent_i*1.5*spacing, -layer_level\n",
    "                cx, cy = child_i*1.5*spacing, -prev_layer_level\n",
    "                # segments.append(((px, py), (cx, cy)))\n",
    "                to_print.append((child_state, f\"{count}\\n{value:.2f}\"))\n",
    "        layer_row += 1\n",
    "        layer = next_layer\n",
    "    draw_layer(ax, to_print, layer_level, spacing)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.tick_params(axis='both', which='both', bottom=False, top=False, left=False, labelbottom=False)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_frame_on(False)\n",
    "ax.set_aspect('equal')\n",
    "plot_mcts(ax, mcts, 0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "data = board.reshape((3,3))\n",
    "\n",
    "# create discrete colormap\n",
    "cmap = colors.ListedColormap(['red', 'grey', 'blue'])\n",
    "bounds = [-2, -0.5, 0.5, 2]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "def box_pos(num_box, layer_level, spacing, offset=0.0):\n",
    "    offset = spacing*offset\n",
    "    draw_pos = [(i*1.5*spacing+offset, -layer_level+offset) for i in range(num_box)]\n",
    "    return draw_pos\n",
    "\n",
    "def draw_edges(ax, dp1, dp2, edges):\n",
    "    # print(edges)\n",
    "    segments = []\n",
    "    for p, c in edges:\n",
    "        segments.append(((dp1[p], dp2[c])))\n",
    "    # print(\"segments\", segments)\n",
    "    line_segments = LineCollection(segments, linestyles='solid', color='red', zorder=0)\n",
    "    ax.add_collection(line_segments)\n",
    "\n",
    "def draw_layer(ax, to_print, draw_pos, spacing):\n",
    "    #draw layer\n",
    "    gx = np.linspace(0, 0.6, 4)\n",
    "    for pos, t in zip(draw_pos, to_print):\n",
    "        x, y = pos\n",
    "        state, text = t\n",
    "        state = state.reshape((3,3))\n",
    "        ax.pcolormesh(gx*spacing+x, gx*spacing+y, state, shading='flat', cmap=cmap)\n",
    "        ax.text(x+0.65*spacing, y, text)\n",
    "\n",
    "\n",
    "def plot_mcts(ax, mcts, root_index): \n",
    "    layer = [root_index]\n",
    "    value, count = mcts.action_mean_value[root_index].max(), mcts.visit_count[root_index]\n",
    "    to_print = [(mcts.state[root_index], f\"Visits: {count}\\nValue: {value:.2f}\")]\n",
    "\n",
    "    to_print_l = [to_print]\n",
    "    edges_l = []\n",
    "\n",
    "    layer_row = 0\n",
    "    while len(layer) > 0:\n",
    "        next_layer = []\n",
    "        to_print = []\n",
    "        edges = []\n",
    "        seen_states = {}\n",
    "        for parent_i, state_index in enumerate(layer):\n",
    "            state = mcts.state[state_index]\n",
    "            visited_actions, = np.nonzero(mcts.action_visits[state_index])\n",
    "            visit_counts = mcts.action_visits[state_index][visited_actions]\n",
    "            action_values = mcts.action_mean_value[state_index, visited_actions]\n",
    "            for action, count, value in zip(visited_actions, visit_counts, action_values):\n",
    "                child_state = flip_board(get_next_state(state, action))\n",
    "                if(child_state.tobytes() in seen_states):\n",
    "                    child_i = seen_states[child_state.tobytes()]\n",
    "                    edges.append((parent_i, child_i))\n",
    "                    continue\n",
    "\n",
    "                if(child_state.tobytes() in mcts.state_lookup):\n",
    "                    child_index = mcts.state_lookup[child_state.tobytes()]\n",
    "                    next_layer.append(child_index)\n",
    "                # if(layer_row % 2 == 0):\n",
    "                #     child_state = flip_board(child_state)\n",
    "                    # value *= -1\n",
    "                \n",
    "                child_i = len(to_print)\n",
    "                edges.append((parent_i, child_i))\n",
    "                seen_states[child_state.tobytes()] = child_i\n",
    "                to_print.append((child_state, f\"{count}\\n{value:.2f}\"))\n",
    "        \n",
    "        to_print_l.append(to_print)\n",
    "        edges_l.append(edges)\n",
    "        layer_row += 1\n",
    "        layer = next_layer\n",
    "    \n",
    "    spacing_l = np.array([min(0.3, 2/len(to_print)) for to_print in to_print_l])\n",
    "    layer_level_l = np.cumsum(spacing_l+0.5)\n",
    "\n",
    "    draw_pos = box_pos(len(to_print_l[0]), layer_level_l[0], spacing_l[0], offset=0.3)\n",
    "    for i in range(1, len(to_print_l)):\n",
    "        next_draw_pos = box_pos(len(to_print_l[i]), layer_level_l[i], spacing_l[i], offset=0.3)\n",
    "        # print(len(draw_pos), len(next_draw_pos))\n",
    "        draw_edges(ax, draw_pos, next_draw_pos, edges_l[i-1])\n",
    "        draw_pos = next_draw_pos\n",
    "    \n",
    "    for i in range(len(to_print_l)):\n",
    "        draw_pos = box_pos(len(to_print_l[i]), layer_level_l[i], spacing_l[i])\n",
    "        draw_layer(ax, to_print_l[i], draw_pos, spacing_l[i])\n",
    "    # draw_pos = box_pos(len(to_print), layer_level, spacing)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.tick_params(axis='both', which='both', bottom=False, top=False, left=False, labelbottom=False)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_frame_on(False)\n",
    "ax.set_aspect('equal')\n",
    "plot_mcts(ax, mcts, 0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mcts.action_mean_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134eaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play_episode(model, num_sims=50, temp_threshold=6):\n",
    "    train_examples = []\n",
    "    board = init_board()\n",
    "    step = 0\n",
    "    while True:\n",
    "        mcts = MCTS()\n",
    "        root_index = mcts.mcts_eval(board, model, num_sims=num_sims)\n",
    "        \n",
    "        temp = int(step < temp_threshold)\n",
    "        pi = mcts.get_action_prob(root_index, temperature=temp)\n",
    "\n",
    "        train_examples.append((board, pi, step))\n",
    "\n",
    "        action = np.random.choice(ACTION_DIM, p=pi)\n",
    "        # print(\"action\", action)\n",
    "        \n",
    "        board = flip_board(get_next_state(board, action))\n",
    "\n",
    "        #Reward is always negative because board is flipped after move. \n",
    "        r, game_over = get_reward(board) \n",
    "        if(game_over):\n",
    "            return [(b, p, (-1)**(step-s)) for b, p, s in train_examples]\n",
    "        step += 1\n",
    "\n",
    "def batch_examples(train_examples):\n",
    "    state_batch = jnp.stack([t[0] for t in train_examples])\n",
    "    pa_batch = jnp.stack([t[1] for t in train_examples])\n",
    "    r_batch = jnp.stack([t[2] for t in train_examples]).reshape((-1, 1))\n",
    "    return state_batch, pa_batch, r_batch\n",
    "\n",
    "train_examples = self_play_episode(toy_model, num_sims=1000)\n",
    "state_b, pa_b, r_b = batch_examples(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate, momentum):\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  model = TTTModel()\n",
    "  params = model.init(rng, jnp.ones([1, 9]))['params']\n",
    "\n",
    "  tx = optax.sgd(learning_rate, momentum)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, state_b, pa_b, r_b):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits, exp_value = TTTModel().apply({'params': params}, state_b)\n",
    "    loss = optax.softmax_cross_entropy(logits, pa_b).mean() + jnp.square(r_b - exp_value).mean()\n",
    "    return loss, (logits, exp_value)\n",
    "  grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "  grads, aux = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  # metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "  return state#, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e72bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(rng, 0.01, 0.1)\n",
    "for i in range(150):\n",
    "    print(f\"Batch {i}\")\n",
    "    train_examples = []\n",
    "    for i in range(10):\n",
    "        train_examples += self_play_episode(lambda x: model_agent(x, {'params': state.params}), num_sims=150)\n",
    "    state_b, pa_b, r_b = batch_examples(train_examples)\n",
    "    state = train_step(state, state_b, pa_b, r_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57afba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "board = np.array([1, -1, 0, -1, 1, 0, -1, 0, -1])\n",
    "# board = np.zeros_like(board)\n",
    "print(board.reshape((3,3)))\n",
    "model = TTTModel()\n",
    "action_prior, value = model.apply({'params': state.params}, board)\n",
    "print(action_prior.reshape((3,3)), value)\n",
    "\n",
    "def learned_agent(board):\n",
    "    prior = model_agent(board, {'params': state.params})[0]\n",
    "    prior *= get_valid_mask(board)\n",
    "    return np.argmax(prior)\n",
    "\n",
    "def random_agent(board):\n",
    "    valid_mask = np.float32(get_valid_mask(board))\n",
    "    valid_mask /= valid_mask.sum()\n",
    "    return np.random.choice(board.shape[0], p=valid_mask)\n",
    "\n",
    "def mcts_agent(board):\n",
    "    mcts = MCTS()\n",
    "    root_index = mcts.mcts_eval(board, lambda x: model_agent(x, {'params': state.params}), num_sims=100)\n",
    "    pi = mcts.get_action_prob(root_index, temperature=0)\n",
    "    if(np.min(pi) < 0):\n",
    "        print(board.reshape((3,3)))\n",
    "        print(pi)\n",
    "        assert False\n",
    "    action = np.random.choice(ACTION_DIM, p=pi)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb77dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts_rand_agent(board):\n",
    "    mcts = MCTS()\n",
    "    root_index = mcts.mcts_eval(board, toy_model, num_sims=100)\n",
    "    pi = mcts.get_action_prob(root_index, temperature=0)\n",
    "    if(np.min(pi) < 0):\n",
    "        print(board.reshape((3,3)))\n",
    "        print(pi)\n",
    "        assert False\n",
    "    action = np.random.choice(ACTION_DIM, p=pi)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_match(agent1, agent2):\n",
    "    board = init_board()\n",
    "    agents = [agent1, agent2]\n",
    "    step = 0\n",
    "    while True:\n",
    "        a = agents[step%2](board)\n",
    "        board = flip_board(get_next_state(board, a))\n",
    "        reward, game_over = get_reward(board)\n",
    "        # print(step, board)\n",
    "        if(game_over):\n",
    "            return reward**step\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots()\n",
    "match_results = []\n",
    "for i in range(100):\n",
    "    match_results.append(play_match(mcts_rand_agent, random_agent))\n",
    "counts, bins = np.histogram(match_results, bins=3)\n",
    "axis.bar([-1, 0, 1], counts, tick_label=['Loss', 'Tie', 'Win'])\n",
    "axis.set_title(\"Random MCTS vs Random\")\n",
    "fig.show()\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "match_results = []\n",
    "for i in range(100):\n",
    "    match_results.append(play_match(mcts_agent, random_agent))\n",
    "counts, bins = np.histogram(match_results, bins=3)\n",
    "axis.bar([-1, 0, 1], counts, tick_label=['Loss', 'Tie', 'Win'])\n",
    "axis.set_title(\"MCTS vs Random\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95256df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(6.5,8))\n",
    "\n",
    "match_results = []\n",
    "for i in range(1000):\n",
    "    match_results.append(play_match(learned_agent, random_agent))\n",
    "counts, bins = np.histogram(match_results, bins=3)\n",
    "axes[0, 0].bar([-1, 0, 1], counts, tick_label=['Loss', 'Tie', 'Win'])\n",
    "axes[0, 0].set_title(\"Learned vs Random\")\n",
    "\n",
    "match_results = []\n",
    "for i in range(1000):\n",
    "    match_results.append(play_match(random_agent, random_agent))\n",
    "counts, bins = np.histogram(match_results, bins=3)\n",
    "axes[0, 1].bar([-1, 0, 1], counts, tick_label=['Loss', 'Tie', 'Win'])\n",
    "axes[0, 1].set_title(\"Random vs Random\")\n",
    "\n",
    "match_results = []\n",
    "for i in range(1000):\n",
    "    match_results.append(play_match(random_agent, learned_agent))\n",
    "counts, bins = np.histogram(match_results, bins=3)\n",
    "axes[1, 0].bar([-1, 0, 1], counts, tick_label=['Loss', 'Tie', 'Win'])\n",
    "axes[1, 0].set_title(\"Random vs Learned\")\n",
    "\n",
    "# match_results = []\n",
    "# for i in range(1000):\n",
    "#     match_results.append(play_match(learned_agent, learned_agent))\n",
    "# counts, bins = np.histogram(match_results, bins=3)\n",
    "# axes[1, 1].bar([-1, 0, 1], counts, tick_label=['Loss', 'Tie', 'Win'])\n",
    "# axes[1, 1].set_title(\"Learned vs Learned\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c26edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_results = []\n",
    "for i in range(1000):\n",
    "    match_results.append(play_match(random_agent, random_agent))\n",
    "plt.hist(match_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
